{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "empty-classics",
   "metadata": {},
   "source": [
    "## load libraries and written functions\n",
    "\n",
    "Note: If you do not install the RDKit python package, please go to the verano.py file and manually comment out any codes/imports relating to RDKit to avoid import errors in the line below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verano import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-drilling",
   "metadata": {},
   "source": [
    "# Load dataset and descriptor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets=['Delaney','Test']\n",
    "\n",
    "md_suffix=['67','PCAS','UD','PFAS','PC']\n",
    "\n",
    "md_names=['RF & 67 Descriptors',\n",
    "          'RF & PCA selected',\n",
    "          'RF & Uncorrelated',\n",
    "          'RF & PFA selected',\n",
    "          'RF & Principal components',\n",
    "          'Single decision tree']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-waste",
   "metadata": {},
   "source": [
    "## Load descriptor names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "file='./data/descriptors.txt'\n",
    "\n",
    "with open(file,'r') as lines:\n",
    "    descriptors=lines.readlines()\n",
    "\n",
    "descriptors=[x.strip() for x in descriptors] \n",
    "\n",
    "print('The first five descriptors are:',descriptors[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-cradle",
   "metadata": {},
   "source": [
    "# Create standard descriptors\n",
    "\n",
    "## Import datasets, create standard descriptors\n",
    "\n",
    "Run this part to create the standard descriptors using the RDKit software library or go to section 1.3 to load the saved data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['name','logS','smiles']\n",
    "data_up=[]\n",
    "\n",
    "count=0\n",
    "for i,dataset in enumerate(datasets):\n",
    "    print('importing dataset',dataset)\n",
    "    vars()[dataset]=pd.read_csv('./data/{0}.csv'.format(dataset))\n",
    "    vars()[dataset].columns=cols\n",
    "    # Convert all smiles strings to canonical format\n",
    "    vars()[dataset]['smiles']=canonicals(vars()[dataset].smiles)\n",
    "    vars()[dataset]['data']=[i]*len(vars()[dataset])\n",
    "    data_up.append(vars()[dataset])\n",
    "    count+=len(vars()[dataset])\n",
    "\n",
    "print('Number of total compounds across all sets:',count)\n",
    "\n",
    "# create one dataframe with all solubility data\n",
    "mol_data=pd.concat(data_up,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-mortgage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to rdkit format and add hydrogens\n",
    "mol_rdkit=SMILES2MOLES(mol_data.smiles)\n",
    "mol_h=ADDH_MOLES(mol_rdkit)\n",
    "\n",
    "# Get 67 standard descriptors\n",
    "mol_SD,del_idx=STDD(mol_h)\n",
    "mol_descriptors=pd.DataFrame(data=mol_SD)\n",
    "\n",
    "# Delete molecules whose descriptors could not be obtained\n",
    "mol_data=mol_data.drop(del_idx)\n",
    "mol_data=mol_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any molecules having a nan as a value\n",
    "mol_data,mol_descriptors=remove_nans(mol_data,mol_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-transport",
   "metadata": {},
   "source": [
    "## Save data\n",
    "\n",
    "Comment in this code to save the descriptors that were created to avoid having to reproduce them again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save descriptors to save time \n",
    "# pkl.dump(mol_data,open('./data/pa_data.pkl','wb'))\n",
    "# pkl.dump(mol_descriptors,open('./data/pa_descriptors.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-breast",
   "metadata": {},
   "source": [
    "## Load saved data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mol_data=pkl.load(open('./data/pa_data.pkl','rb'))\n",
    "# mol_descriptors=pkl.load(open('./data/pa_descriptors.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-complement",
   "metadata": {},
   "source": [
    "## Create lists to store the indices of the training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx=list(mol_data.index[mol_data['data']==1])\n",
    "train_idx=list(mol_data.index[mol_data['data']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-coast",
   "metadata": {},
   "source": [
    "## Distribution of train/test set values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mean=mol_data.loc[test_idx].logS.values.mean()\n",
    "test_std=mol_data.loc[test_idx].logS.values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_values=[]\n",
    "for dataset in ['train','test']:\n",
    "    logs_values.append(mol_data.iloc[vars()[dataset+'_idx']].logS.values)\n",
    "    print('{0}ing set has mean logS of {1}, std of {2}'.\\\n",
    "          format(dataset,\n",
    "                 round(np.mean(logs_values[-1]),2),\n",
    "                 round(np.std(logs_values[-1]),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=15\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.ylim((-15,3))\n",
    "plt.ylabel('log $S$',fontsize=fs)\n",
    "plt.xlabel('Dataset',fontsize=fs)\n",
    "ax=sns.boxplot(data=logs_values,showmeans=True,\n",
    "              meanprops={\"marker\":\"o\",\n",
    "                       \"markerfacecolor\":\"white\", \n",
    "                       \"markeredgecolor\":\"black\",\n",
    "                      \"markersize\":\"5\"})\n",
    "ax.set_xticklabels(datasets,fontsize=fs-4)\n",
    "ax.set_yticklabels(ax.get_yticks(),size=fs-4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-norman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "velvet-stuart",
   "metadata": {},
   "source": [
    "# Principal component analysis\n",
    "\n",
    "PCA is applied on the standardized training data. The model is then used to transform the standardized test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MD=mol_descriptors.iloc[train_idx]\n",
    "test_MD=mol_descriptors.iloc[test_idx]\n",
    "\n",
    "# Normalize or 'z-score' features\n",
    "ss=StandardScaler()\n",
    "train_scaled=ss.fit_transform(train_MD)\n",
    "test_scaled=ss.transform(test_MD)\n",
    "\n",
    "# PCA model\n",
    "pca=PCA()\n",
    "project_train=pca.fit_transform(train_scaled)\n",
    "project_test=pca.transform(test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-adventure",
   "metadata": {},
   "source": [
    "## Proportion of variance explained \n",
    "\n",
    "Using the PCA model, we can see how many principal components (PCs) are required to have 90% of the cumulative proportion of variance explained (PVE) and the individual PVE values for those PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_pc=np.where(np.cumsum(pca.explained_variance_ratio_)>=0.9)[0][1]\n",
    "print('90% variance explained by first {} PCs with each having PVE values of:'.format(N_pc))\n",
    "print(pca.explained_variance_ratio_[:N_pc])\n",
    "\n",
    "print('\\nCumulative PVE of first {} PCs:'.format(N_pc))\n",
    "print(np.cumsum(pca.explained_variance_ratio_)[:N_pc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-timber",
   "metadata": {},
   "source": [
    "The PVE and cumulative PVE can be visualized using a scree plot which shows these values for the first 30 PCs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=18\n",
    "N_plot=30\n",
    "fig,axes=plt.subplots(1,2,figsize=(12,5))\n",
    "\n",
    "axes[0].plot(range(1,N_plot+1),pca.explained_variance_ratio_[:N_plot],'-o')\n",
    "axes[0].set_ylabel('Proportion variance explained',fontsize=fs-2)\n",
    "axes[0].set_xlabel('Principal Component',fontsize=fs-2)\n",
    "\n",
    "axes[1].plot(range(1,N_plot+1),np.cumsum(pca.explained_variance_ratio_)[:N_plot],'-o')\n",
    "axes[1].set_xlim(0,N_plot+1)\n",
    "axes[1].set_ylim(0.36,1.01)\n",
    "axes[1].set_ylabel('Cumulative prop variance explained',fontsize=fs-2)\n",
    "axes[1].set_xlabel('Principal Component',fontsize=fs-2)\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].set_xticks(np.linspace(0,N_plot,16))\n",
    "    for tick in axes[i].xaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(fs*0.65) \n",
    "    for tick in axes[i].yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(fs*0.65) \n",
    "        \n",
    "plt.tight_layout(rect=[0,0.03,1,0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-sport",
   "metadata": {},
   "source": [
    "## Distribution of train/test datasets based on two principal components\n",
    "\n",
    "We can compare how the training and test datasets are distributed using the information summarized within a pair of PCs and see how solubility relates to both via side-by-side plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the values of pc1 and pc2 to visualize other PCs\n",
    "\n",
    "pc1=0\n",
    "pc2=1\n",
    "\n",
    "fs=18\n",
    "mapcolor='ocean_r' \n",
    "vmin=-11.5\n",
    "vmax=1.5\n",
    "\n",
    "projections=[project_train,project_test]\n",
    "fig,axes=plt.subplots(1,2,figsize=(16*0.75,7*0.75))\n",
    "for i,dataset in enumerate(['train','test']):\n",
    "    ax=axes[i]\n",
    "    projection=projections[i]\n",
    "    x=projection[:,pc1]\n",
    "    y=projection[:,pc2]\n",
    "    c=mol_data.logS[vars()[dataset+'_idx']]\n",
    "    vars()[dataset+'plot']=ax.scatter(x,y,\n",
    "                    edgecolor='none',\n",
    "                    alpha=0.8,\n",
    "                    c=c,\n",
    "                    cmap=plt.cm.get_cmap(mapcolor),\n",
    "                    s=55,vmin=vmin,vmax=vmax)\n",
    "    ax.set_xlabel('Principal Component {}'.format(pc1+1),fontsize=fs-2)\n",
    "    ax.set_ylabel('Principal Component {}'.format(pc2+1),fontsize=fs-2)\n",
    "    cbar=plt.colorbar(vars()[dataset+'plot'],ax=ax,fraction=0.03,pad=0.055)\n",
    "    cbar.ax.tick_params(labelsize=11)\n",
    "    cbar.ax.set_xlabel('log $S$',fontsize=fs*0.8)\n",
    "    ax.set_title('{}'.format(datasets[i]),fontsize=fs)\n",
    "    ax.set_xlim(-15,40)\n",
    "    ax.set_ylim(-15,15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-danger",
   "metadata": {},
   "source": [
    "## Descriptor loadings for each component\n",
    "\n",
    "The individual PCs can be analyzed to see which descriptors are weighted the most based on their absolute 'loadings' with larger absolute loadings indicating a higher influence of that descriptor for that PC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the absolute loadings \n",
    "loadings=pd.DataFrame()\n",
    "loadings_mat=abs(pca.components_)\n",
    "\n",
    "for j in range(pca.n_components_):\n",
    "    \n",
    "    sorted_feats=[]\n",
    "    for i in loadings_mat[j,:].argsort()[::-1]:\n",
    "        sorted_feats.append((descriptors[i],loadings_mat[j,:][i]))\n",
    "        \n",
    "    loadings['PC'+str(j+1)]=sorted_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-resort",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loadings.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-fields",
   "metadata": {},
   "source": [
    "To determine how each of the descriptors relate to one another, we can use the loading values from a pair of PCs to create feature vectors that are visualized in a biplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the loadings (but without absolute value)\n",
    "loadings=pd.DataFrame()\n",
    "loadings_mat=pca.components_\n",
    "\n",
    "for j in range(pca.n_components_):\n",
    "    \n",
    "    sorted_feats=[]\n",
    "    for i in loadings_mat[j,:].argsort()[::-1]:\n",
    "        sorted_feats.append((descriptors[i],loadings_mat[j,:][i]))\n",
    "        \n",
    "    loadings['PC'+str(j+1)]=sorted_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loadings for individual descriptors to create biplot\n",
    "pc1_idx=[]\n",
    "pc2_idx=[]\n",
    "\n",
    "for i in range(67):\n",
    "    pc1_idx.append(loadings['PC1'][i][0])\n",
    "    pc2_idx.append(loadings['PC2'][i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vector of loadings for four descriptors in v1\n",
    "# Normalize each vector in v2\n",
    "v1=[]\n",
    "v2=[]\n",
    "\n",
    "# Can change these descriptor names to look at other ones\n",
    "desctype=['MolWt','MolMR','MolLogP','TPSA']\n",
    "\n",
    "for i in desctype:\n",
    "    idx1=pc1_idx.index(i)\n",
    "    idx2=pc2_idx.index(i)\n",
    "    v1.append([loadings['PC1'][idx1][1],loadings['PC2'][idx2][1]])\n",
    "    v2.append(v1[-1]/np.linalg.norm(v1[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(desctype)):\n",
    "    print('The feature and normalized feature vectors for {}\\n'.\n",
    "          format(desctype[i]),v1[i],v2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-weather",
   "metadata": {},
   "source": [
    "The dot product between two normalized feature vectors is indicative of their relationship:\n",
    "\n",
    "1. 0: descriptors are independent\n",
    "2. -1: linearly negatively correlated\n",
    "3. 1: linearly positively correlated\n",
    "\n",
    "For MolMR and MolWt, we see that they are very positively correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(v2[0],v2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-pierce",
   "metadata": {},
   "source": [
    "These vectors can be analyzed on a biplot and we can also see how the first two PCs compare to different descriptors in side-by-side plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of lipophilicity against exact molecular weight\n",
    "plot_idx=train_idx\n",
    "feature_idx1=6\n",
    "feature_idx2=4\n",
    "s1=1\n",
    "s2=-1\n",
    "pc1=0\n",
    "pc2=1\n",
    "\n",
    "fs=18\n",
    "fig,axes=plt.subplots(1,2,figsize=(16*0.75,7*0.75))\n",
    "ax=axes[0]\n",
    "x=project_train[:,pc1]\n",
    "y=project_train[:,pc2]\n",
    "c=mol_data.logS[plot_idx]\n",
    "del_plot=ax.scatter(x,y,\n",
    "                edgecolor='none',\n",
    "                alpha=0.8,\n",
    "                c=c,\n",
    "                cmap=plt.cm.get_cmap(mapcolor),\n",
    "                s=55,vmin=vmin,vmax=vmax)\n",
    "ax.set_xlabel('First Principal Component',fontsize=fs-2)\n",
    "ax.set_ylabel('Second Principal Component',fontsize=fs-2)\n",
    "cbar=plt.colorbar(del_plot,ax=ax,fraction=0.03,pad=0.05)\n",
    "cbar.ax.tick_params(labelsize=11)\n",
    "cbar.ax.set_xlabel('log $S$',fontsize=fs*0.8)\n",
    "ax.set_xlim(-12.5,30.5)\n",
    "ax.set_ylim(-15,15)\n",
    "\n",
    "\n",
    "V=np.array(v2)\n",
    "c=['green','red','gold','dodgerblue']\n",
    "origin=np.array([[0,0,0],[0,0,0]])\n",
    "for i in range(4):\n",
    "    ax.quiver(*origin,V[i,0],V[i,1],color=c[i],scale=2.5,label=desctype[i])\n",
    "ax.set_xlabel('First principal component',fontsize=fs-2)\n",
    "ax.set_ylabel('Second principal component',fontsize=fs-2)\n",
    "ax.legend(loc=4)\n",
    "\n",
    "featplot=axes[1].scatter(s1*mol_descriptors.iloc[plot_idx][feature_idx1],\n",
    "                s2*mol_descriptors.iloc[plot_idx][feature_idx2],\n",
    "            edgecolor='none',\n",
    "            alpha=0.8,\n",
    "            c=mol_data.iloc[plot_idx]['logS'],\n",
    "            s=55,\n",
    "            cmap=plt.cm.get_cmap(mapcolor),vmin=vmin,vmax=vmax) #,6))\n",
    "axes[1].set_ylabel(descriptors[feature_idx2],fontsize=fs-2)\n",
    "axes[1].set_xlabel(descriptors[feature_idx1],fontsize=fs-2)\n",
    "cbar=plt.colorbar(featplot,fraction=0.03,pad=0.05,ax=axes[1])\n",
    "cbar.ax.tick_params(labelsize=0.65*fs)\n",
    "cbar.ax.set_xlabel('log $S$',fontsize=fs*0.8)\n",
    "axes[1].set_xlim(-50,800)\n",
    "axes[1].set_ylim(-10,10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-implementation",
   "metadata": {},
   "source": [
    "# Correlation analysis on Delaney dataset\n",
    "\n",
    "Looking at the Pearson correlation coefficient (PCC) between pairs of descriptors presents another way to do dimensionality reduction. Here the heatmap shows the PCC between pairs of descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_Delaney=mol_descriptors.iloc[train_idx]\n",
    "corr_mat=md_Delaney.corr()\n",
    "corr_mat.columns=descriptors\n",
    "\n",
    "plt.figure(figsize=(9*1.5,7.5*1.5),facecolor='w',edgecolor='k')\n",
    "ax=sns.heatmap(corr_mat,linewidth=0.25,cmap='RdBu',\n",
    "               cbar_kws={'shrink': 0.5,'label':'Pearson correlation'},\n",
    "               xticklabels=corr_mat.columns,yticklabels=corr_mat.columns,\n",
    "              vmin=-1,vmax=1)\n",
    "ax.figure.axes[-1].yaxis.label.set_size(12)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-batch",
   "metadata": {},
   "source": [
    "## Find features with correlations greater than 0.80\n",
    "\n",
    "We can find features that have absolute PCC greater than 0.80. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.8\n",
    "\n",
    "corr_np=np.array(abs(corr_mat))\n",
    "np.fill_diagonal(corr_np,0)\n",
    "\n",
    "# Where in the correlation matrix are values above threshold\n",
    "corr_80=corr_np>=threshold\n",
    "\n",
    "# List of the sets of correlated features\n",
    "merge_corr=[]\n",
    "\n",
    "for i in range(67):\n",
    "    corrs=[]\n",
    "    column=corr_80[:,i]\n",
    "    if np.max(column)==1:\n",
    "        corrs.append((descriptors[i],i))\n",
    "        feats=np.where(column==1)\n",
    "        for f in feats[0]:\n",
    "            corrs.append((descriptors[f],f,corr_np[i,f]))\n",
    "        merge_corr.append(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-belize",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices=set()\n",
    "\n",
    "for sets in merge_corr:\n",
    "    for i,s in enumerate(sets):\n",
    "        if i==0:\n",
    "            print('Correlations for feature: ',s)\n",
    "        else:\n",
    "            print(s)\n",
    "        indices.add(s[1])\n",
    "    print('\\n')\n",
    "    \n",
    "indices=list(indices)\n",
    "\n",
    "print('Number of features correlated with Pearson coeff >= {0}: {1}'.format(threshold,len(indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-mortgage",
   "metadata": {},
   "source": [
    "As many of the descriptors are highly correlated with another, we can remove those descriptors that have the greatest average PCC with all the rest until there is no longer between-descriptor PCC values greater than 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "Delaney_UD=uncorr_descriptor(mol_descriptors,train_idx,test_idx)\n",
    "corr_mat=Delaney_UD.corr()\n",
    "descriptors_uncorr=[descriptors[int(i)] for i in Delaney_UD.keys()]\n",
    "corr_mat.columns=descriptors_uncorr\n",
    "\n",
    "plt.figure(figsize=(9*0.8,7.5*0.8),facecolor='w',edgecolor='k')\n",
    "ax=sns.heatmap(corr_mat,linewidth=0.5,cmap='RdBu',\n",
    "               xticklabels=corr_mat.columns,yticklabels=corr_mat.columns,\n",
    "               cbar_kws={'label':'Pearson correlation','shrink':0.6},\n",
    "               vmin=-1,vmax=1)\n",
    "ax.figure.axes[-1].yaxis.label.set_size(12)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-moldova",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wrong-paintball",
   "metadata": {},
   "source": [
    "# Prediction models using random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-brazilian",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "The PCA and correlation analysis can be utilized to do dimensionality reduction and feature selection. Four subsets and versions of the original descriptors are created from that analysis:\n",
    "\n",
    "1. Principal components representing 90% of the cumulative PVE\n",
    "2. A subset of descriptors chosen based on their loadings for PCs representing 80% of the cumulative PVE\n",
    "3. Principal feature selection based on the number of PCs representing 90% of the cumulative PVE \n",
    "4. Correlation analysis from section 3.1 \n",
    "\n",
    "5-Fold cross-validation will be used to determine which of those four along with the full 67 descriptors will be effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-oxford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "cv_results={}\n",
    "\n",
    "# Defining splits for cross validation\n",
    "kf=KFold(n_splits=5,random_state=44,shuffle=True)\n",
    "X=mol_descriptors.iloc[train_idx]\n",
    "Y=mol_data.iloc[train_idx]\n",
    "\n",
    "for j,dtype in enumerate(md_suffix):\n",
    "    print('Cross validation on {}'.format(md_names[j]))\n",
    "    \n",
    "    for i,[train_index,test_index] in enumerate(kf.split(X,Y)):\n",
    "\n",
    "        train_index=list(train_index)\n",
    "        test_index=list(test_index)\n",
    "        \n",
    "        X_desc=get_descriptor(dtype,X,train_index,test_index)\n",
    "        Xtrain_kf=X_desc.loc[train_index]\n",
    "        Xtest_kf=X_desc.loc[test_index]\n",
    "        Ytrain_kf=mol_data.iloc[train_index]['logS'].values\n",
    "        Ytest_kf=mol_data.iloc[test_index]['logS'].values\n",
    "        scaler=StandardScaler()\n",
    "        Ytrain_ss=scaler.fit_transform(np.array(Ytrain_kf).reshape(-1,1))\n",
    "        \n",
    "        rf=RandomForestRegressor(n_estimators=500,random_state=10)\n",
    "        rf.fit(Xtrain_kf,np.ravel(Ytrain_ss))\n",
    "        yhat_kf=scaler.inverse_transform(rf.predict(Xtest_kf))\n",
    "        cv_results[dtype+str(i)]=[Ytest_kf,yhat_kf]\n",
    "        print('Completed fold',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-frost",
   "metadata": {},
   "source": [
    "### Cross-validation results\n",
    "\n",
    "Consolidate cross-validation the mean results of each fold into a dataframe. Results are evaluated based on two metrics:\n",
    "\n",
    "1. RMSE\n",
    "2. Percent accuracy based on number of predictions being within $\\pm$ of 0.7 log$S$ units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_columns=['Descriptor','RMSE','RMSE/SD','logS_pm0.7']\n",
    "cv_metrics=pd.DataFrame(columns=cv_columns)\n",
    "     \n",
    "for i,dtype in enumerate(md_suffix):\n",
    "    kf_RMSE=[]\n",
    "    kf_RMSE_std=[]\n",
    "    kf_logS_pm7=[]\n",
    "\n",
    "    r_idx=5*i+j\n",
    "    row=[md_names[i]]\n",
    "\n",
    "    for k in range(5):\n",
    "        Ytest_kf=cv_results[dtype+str(k)][0]\n",
    "        yhat_kf=cv_results[dtype+str(k)][1]\n",
    "        diff=abs(Ytest_kf-yhat_kf)\n",
    "        lt7=np.sum(diff<=0.7)/len(Ytest_kf)*100\n",
    "\n",
    "        kf_RMSE.append(np.sqrt(mse(Ytest_kf,yhat_kf)))\n",
    "        kf_RMSE_std.append(kf_RMSE[-1]/np.std(Ytest_kf))\n",
    "        kf_logS_pm7.append(lt7)\n",
    "\n",
    "    row.append(np.mean(kf_RMSE))\n",
    "    row.append(np.mean(kf_RMSE_std))\n",
    "    row.append(np.mean(kf_logS_pm7))\n",
    "    cv_metrics.loc[r_idx]=row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_metrics.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-truck",
   "metadata": {},
   "source": [
    "## Random forest models\n",
    "\n",
    "Having determine which of the descriptors are the most suitable, we can choose the best ones for predicting on the test set. Since we're only dealing with one algorithm and one training dataset here, we can quickly run the RF model on all five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store results\n",
    "rf_results={}\n",
    "\n",
    "for i,dtype in enumerate(md_suffix):\n",
    "    print('Fitting RF for {}'.format(md_names[i]))\n",
    "    vars()['Delaney_'+dtype]=get_descriptor(dtype,mol_descriptors,train_idx,test_idx)\n",
    "    Xtrain=vars()['Delaney_'+dtype].iloc[train_idx]\n",
    "    Ytrain=mol_data['logS'].iloc[train_idx]\n",
    "    Xtest=vars()['Delaney_'+dtype].iloc[test_idx]\n",
    "    Ytest=mol_data['logS'].iloc[test_idx]\n",
    "    rf_results['Delaney_'+dtype]=rf_model(Xtrain,Xtest,Ytrain,Ytest,trees=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-netscape",
   "metadata": {},
   "source": [
    "For purposes of comparison, we train a single decision tree model to make predictions and see how much predictive accuracy is gained from using an ensemble-based method such as RFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single tree using 67 descriptors for comparison\n",
    "Xtrain=Delaney_67.iloc[train_idx]\n",
    "Ytrain=mol_data['logS'].iloc[train_idx]\n",
    "Xtest=Delaney_67.iloc[test_idx]\n",
    "Ytest=mol_data['logS'].iloc[test_idx]\n",
    "Delaney_tree=dtree_model(Xtrain,Xtest,Ytrain,Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-rhythm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "competent-tennessee",
   "metadata": {},
   "source": [
    "### Model results\n",
    "\n",
    "Consolidate RF results and the single decision tree performance into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_columns=['Descriptor','Length','RMSE','RMSE/SD','logS_pm0.7']\n",
    "rf_metrics=pd.DataFrame(columns=rf_columns)\n",
    "yhats=[]\n",
    "\n",
    "for i,dtype in enumerate(md_suffix):\n",
    "    \n",
    "    yhats.append(rf_results['Delaney_'+dtype][0])\n",
    "    diff=abs(yhats[-1]-np.array(Ytest))\n",
    "    row=[md_names[i],\n",
    "         vars()['Delaney_'+dtype].shape[1],\n",
    "         np.sqrt(mse(yhats[-1],Ytest))]\n",
    "    row.append(row[-1]/test_std)\n",
    "    row.append(np.sum(diff<=0.7)/len(Ytest)*100)\n",
    "    rf_metrics.loc[i]=row\n",
    "    \n",
    "    \n",
    "yhats.append(Delaney_tree[0])\n",
    "diff=abs(yhats[-1]-np.array(Ytest))\n",
    "row=[md_names[5],\n",
    "     Delaney_67.shape[1],\n",
    "     np.sqrt(mse(yhats[-1],Ytest))]\n",
    "row.append(row[-1]/test_std)\n",
    "row.append(np.sum(diff<=0.7)/len(Ytest)*100)\n",
    "rf_metrics.loc[5]=row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-dietary",
   "metadata": {},
   "source": [
    "## Plot of model predictions\n",
    "\n",
    "The results of each model are plotted to see how accurate each descriptor/model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-meeting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fs=14\n",
    "lb=-8.5\n",
    "ub=0\n",
    "\n",
    "axlabels=['a','b','c','d','e','f']\n",
    "color='tab:blue'\n",
    "ms=25\n",
    "\n",
    "cols=3\n",
    "rows=2\n",
    "\n",
    "fig,axes=plt.subplots(nrows=rows,ncols=cols,figsize=(12,8.5))\n",
    "for i in range(6):\n",
    "    ax=axes[int(i/cols)][i%cols]\n",
    "    ax.scatter(Ytest,yhats[i],\n",
    "               s=ms,\n",
    "               color=color,\n",
    "               facecolors=color)     \n",
    "    resultsplot_setup(ax,fs,lb,ub,md_names[i],axlabels[i],weight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-mason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rapid-peripheral",
   "metadata": {},
   "source": [
    "# Hard to predict molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=[]\n",
    "for i in range(5):\n",
    "    errors.append(abs(yhats[i]-Ytest))\n",
    "    \n",
    "errors=np.array(errors)\n",
    "MAE=errors.mean(axis=0)\n",
    "\n",
    "sort_idx=np.argsort(MAE)[::-1]\n",
    "err_idx=np.array(test_idx)[sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=mol_data.iloc[test_idx].copy()\n",
    "data_test['MAE']=MAE\n",
    "data_test.sort_values('MAE',ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.scatter(Ytest,errors[i],s=30,color='tab:blue',marker='x',lw=1)\n",
    "\n",
    "plt.ylim(-0.25,5)\n",
    "plt.xlim(-7.25,-0.75)\n",
    "plt.xlabel('log $S$',fontsize=fs-2)\n",
    "plt.ylabel('$abs($error$)$',fontsize=fs-2)\n",
    "plt.setp(ax.get_xticklabels(),fontsize=fs-4)\n",
    "plt.setp(ax.get_yticklabels(),fontsize=fs-4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-comparative",
   "metadata": {},
   "source": [
    "# Uncertainty quantification\n",
    "\n",
    "Since RFs utilize a collection of decision trees, the predictions for the individual trees can be utilized to calculate an uncertainty value regarding each prediction called the 'bootstrap variance'. The variance and standard deviation for each set of predictions are calculated. Based on the standard deviation and assuming normality, a 95% prediction interval can be calculated and visualized to see how reliable is each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=rf_results['Delaney_PCAS'][1]\n",
    "yhat_trees=np.zeros((500,100))\n",
    "\n",
    "for i,tree in enumerate(model.estimators_):\n",
    "    yhat_trees[i,:]=tree.predict(Delaney_PCAS.iloc[test_idx])\n",
    "\n",
    "yh_var=yhat_trees.std(axis=0)\n",
    "yh_std=yhat_trees.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intervals  \n",
    "percent=95\n",
    "left=[]\n",
    "right=[]\n",
    "interval=np.zeros((100,3))\n",
    "interval[:,0]=rf_results['Delaney_UD'][0]\n",
    "for i in range(len(Ytest)):\n",
    "    interval[i,1]=interval[i,0]-yh_std[i]*1.96\n",
    "    interval[i,2]=interval[i,0]+yh_std[i]*1.96\n",
    "    \n",
    "pred_interval=pd.DataFrame(data=interval,\n",
    "                           columns=['logS','lower','upper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sort=np.sort(Ytest)\n",
    "Y_sort_idx=np.array(np.argsort(Ytest))\n",
    "X=range(100)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(X,Y_sort,'o',color='gold',label='Experimental log $S$')\n",
    "plt.plot(X,interval[:,0][Y_sort_idx],'o',color='tab:blue',\n",
    "         label='Predicted log $S$',alpha=0.75)\n",
    "for i in range(100):\n",
    "    lbnd=interval[Y_sort_idx[i],1]\n",
    "    ubnd=interval[Y_sort_idx[i],2]\n",
    "    plt.plot(X[i]*np.ones(20),np.linspace(lbnd,ubnd,20),color='lightblue',lw=4,zorder=0)\n",
    "plt.legend(loc=4)\n",
    "plt.ylabel('log $S$',fontsize=14)\n",
    "plt.xlabel('Ordered molecules',fontsize=14)\n",
    "plt.savefig('uq.png',dpi=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-deadline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "776px",
    "left": "74px",
    "top": "208.133px",
    "width": "244.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
